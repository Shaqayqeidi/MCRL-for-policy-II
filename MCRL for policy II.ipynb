{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "monte carlo for policy II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import sample\n",
    "import math \n",
    "from collections import defaultdict\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_scale=(2365.08,996.88,713.55,1406.84,343.76,3933.12,828.19,2040.95)\n",
    "weibull_shape=(414.16,109.25,79.81,115.21,169.81,143.60,43.83,296.48)\n",
    "tf=(2,6.5,2.5,6,5,3.5,3,3.5)\n",
    "tp=(0.4,5.42,0.625,0.857,1.25,0.7,0.429,0.875)\n",
    "time_interval=5\n",
    "running_time=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset1():\n",
    "    st= [0]*16\n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step (action,st,i): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[i],weibull_shape[i])\n",
    "    if action == 0 :\n",
    "        st[i] +=5\n",
    "        if f <= st[i]:\n",
    "            st[i+8]=1    \n",
    "        else:\n",
    "            st[i+8]=0 \n",
    "            \n",
    "    if action ==1 :\n",
    "            st[i]=0\n",
    "            st[i+8]=0\n",
    "           \n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun(action,st,i) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[i])*tp[i]    \n",
    "    if (st[i+8]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[i])*time_interval * math.ceil(tf[i]/time_interval)\n",
    "    \n",
    "    if (st[i+8]== 0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_POSSIBLE_ACTIONS = [0,1]\n",
    "def policy_using_pi(st, pi):\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi[st][a] for a in ALL_POSSIBLE_ACTIONS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy_using_pi((1,1), pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state , pi,i):\n",
    "    if (state[i+8]==1):\n",
    "        return  1  #repleace with probability 1\n",
    "    else: \n",
    "        st = (state[i],state[i+8])\n",
    "        return policy_using_pi(st, pi)  #epsilon_soft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = (1,0,2,5,1,4,5,0,0,0,0,0,0,0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = (state[1],state[1+8])\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose_action(state , pi ,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Tire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize \n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "pi = np.full((100000 ,2) + (2,),0.5)\n",
    "#pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "i=0\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_ti = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_ti , pi ,1)\n",
    "        s_ti = step(a_t,list(s_ti),i)\n",
    "        G = rewardfun(a_t,s_ti,i)\n",
    "        \n",
    "        seen_state_action_pairs = set()\n",
    "        \n",
    "        s_t = (s_ti[i] , s_ti[i+8] )\n",
    "        state_action = (s_t,a_t)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "            \n",
    "            returns[s_t][a_t] += G\n",
    "            N[s_t][a_t] +=1\n",
    "                \n",
    "            Q[s_t][a_t] = returns[s_t][a_t] /N[s_t][a_t] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "                 \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[s_t][a] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[s_t][a] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2350"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs = step(action , list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize \n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "pi = np.full((100000 ,2) + (2,),0.5)\n",
    "#pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "i=1\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_ti = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_ti , pi ,1)\n",
    "        s_ti = step(a_t,list(s_ti),i)\n",
    "        G = rewardfun(a_t,s_ti,i)\n",
    "        \n",
    "        seen_state_action_pairs = set()\n",
    "        \n",
    "        s_t = (s_ti[i] , s_ti[i+8] )\n",
    "        state_action = (s_t,a_t)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "            \n",
    "            returns[s_t][a_t] += G\n",
    "            N[s_t][a_t] +=1\n",
    "                \n",
    "            Q[s_t][a_t] = returns[s_t][a_t] /N[s_t][a_t] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "                 \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[s_t][a] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[s_t][a] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 955,  960,  965,  970,  975,  980,  985,  990,  995, 1000, 1005])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=1\n",
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs = step(action , list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(time_replace)[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize \n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "pi = np.full((100000 ,2) + (2,),0.5)\n",
    "#pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "i=2\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_ti = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_ti , pi ,1)\n",
    "        s_ti = step(a_t,list(s_ti),i)\n",
    "        G = rewardfun(a_t,s_ti,i)\n",
    "        \n",
    "        seen_state_action_pairs = set()\n",
    "        \n",
    "        s_t = (s_ti[i] , s_ti[i+8] )\n",
    "        state_action = (s_t,a_t)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "            \n",
    "            returns[s_t][a_t] += G\n",
    "            N[s_t][a_t] +=1\n",
    "                \n",
    "            Q[s_t][a_t] = returns[s_t][a_t] /N[s_t][a_t] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "                 \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[s_t][a] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[s_t][a] = (epsilon / len(ALL_POSSIBLE_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([655, 675, 680, 685, 690, 695, 700, 705, 710, 715, 720, 725])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=2\n",
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs = step(action , list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(time_replace)[4] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize \n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "pi = np.full((100000 ,2) + (2,),0.5)\n",
    "#pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "i=3\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_ti = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_ti , pi ,1)\n",
    "        s_ti = step(a_t,list(s_ti),i)\n",
    "        G = rewardfun(a_t,s_ti,i)\n",
    "        \n",
    "        seen_state_action_pairs = set()\n",
    "        \n",
    "        s_t = (s_ti[i] , s_ti[i+8] )\n",
    "        state_action = (s_t,a_t)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "            \n",
    "            returns[s_t][a_t] += G\n",
    "            N[s_t][a_t] +=1\n",
    "                \n",
    "            Q[s_t][a_t] = returns[s_t][a_t] /N[s_t][a_t] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "                 \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[s_t][a] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[s_t][a] = (epsilon / len(ALL_POSSIBLE_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1325, 1355, 1360, 1365, 1370, 1375, 1380, 1385, 1390, 1395, 1400,\n",
       "       1405, 1410, 1415])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=3\n",
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs = step(action , list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1360"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(time_replace)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize \n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "pi = np.full((100000 ,2) + (2,),0.5)\n",
    "#pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "i=4\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_ti = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_ti , pi ,1)\n",
    "        s_ti = step(a_t,list(s_ti),i)\n",
    "        G = rewardfun(a_t,s_ti,i)\n",
    "        \n",
    "        seen_state_action_pairs = set()\n",
    "        \n",
    "        s_t = (s_ti[i] , s_ti[i+8] )\n",
    "        state_action = (s_t,a_t)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "            \n",
    "            returns[s_t][a_t] += G\n",
    "            N[s_t][a_t] +=1\n",
    "                \n",
    "            Q[s_t][a_t] = returns[s_t][a_t] /N[s_t][a_t] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "                 \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[s_t][a] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[s_t][a] = (epsilon / len(ALL_POSSIBLE_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([335, 340, 345, 350])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=4\n",
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs = step(action , list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(time_replace)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize \n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "pi = np.full((100000 ,2) + (2,),0.5)\n",
    "#pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "i=5\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_ti = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_ti , pi ,1)\n",
    "        s_ti = step(a_t,list(s_ti),i)\n",
    "        G = rewardfun(a_t,s_ti,i)\n",
    "        \n",
    "        seen_state_action_pairs = set()\n",
    "        \n",
    "        s_t = (s_ti[i] , s_ti[i+8] )\n",
    "        state_action = (s_t,a_t)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "            \n",
    "            returns[s_t][a_t] += G\n",
    "            N[s_t][a_t] +=1\n",
    "                \n",
    "            Q[s_t][a_t] = returns[s_t][a_t] /N[s_t][a_t] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "                 \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[s_t][a] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[s_t][a] = (epsilon / len(ALL_POSSIBLE_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3790, 3830, 3835, 3840, 3845, 3855, 3865, 3870, 3875, 3880, 3885,\n",
       "       3890, 3895, 3900, 3915, 3925])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=5\n",
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs = step(action , list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3830"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(time_replace)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steering Wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize \n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "pi = np.full((100000 ,2) + (2,),0.5)\n",
    "#pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "i=6\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_ti = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_ti , pi ,1)\n",
    "        s_ti = step(a_t,list(s_ti),i)\n",
    "        G = rewardfun(a_t,s_ti,i)\n",
    "        \n",
    "        seen_state_action_pairs = set()\n",
    "        \n",
    "        s_t = (s_ti[i] , s_ti[i+8] )\n",
    "        state_action = (s_t,a_t)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "            \n",
    "            returns[s_t][a_t] += G\n",
    "            N[s_t][a_t] +=1\n",
    "                \n",
    "            Q[s_t][a_t] = returns[s_t][a_t] /N[s_t][a_t] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "                 \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[s_t][a] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[s_t][a] = (epsilon / len(ALL_POSSIBLE_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([735, 740, 745, 750, 755, 760, 765, 770, 775, 780, 785, 790, 795,\n",
       "       800, 805, 810, 815, 820, 825])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=6\n",
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs = step(action , list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(time_replace)[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shifting Gears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize \n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "pi = np.full((100000 ,2) + (2,),0.5)\n",
    "#pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "i=7\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_ti = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_ti , pi ,1)\n",
    "        s_ti = step(a_t,list(s_ti),i)\n",
    "        G = rewardfun(a_t,s_ti,i)\n",
    "        \n",
    "        seen_state_action_pairs = set()\n",
    "        \n",
    "        s_t = (s_ti[i] , s_ti[i+8] )\n",
    "        state_action = (s_t,a_t)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "            \n",
    "            returns[s_t][a_t] += G\n",
    "            N[s_t][a_t] +=1\n",
    "                \n",
    "            Q[s_t][a_t] = returns[s_t][a_t] /N[s_t][a_t] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "                 \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[s_t][a] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[s_t][a] = (epsilon / len(ALL_POSSIBLE_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1995, 2005, 2010, 2025, 2030, 2035, 2040, 2045, 2050])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=7\n",
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs = step(action , list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1995"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(time_replace)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize \n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "pi = np.full((100000 ,2) + (2,),0.5)\n",
    "#pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "i=7\n",
    "for epi in range(1500 +1):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_ti = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_ti , pi ,1)\n",
    "        s_ti = step(a_t,list(s_ti),i)\n",
    "        G = rewardfun(a_t,s_ti,i)\n",
    "        \n",
    "        seen_state_action_pairs = set()\n",
    "        \n",
    "        s_t = (s_ti[i] , s_ti[i+8] )\n",
    "        state_action = (s_t,a_t)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "            \n",
    "            returns[s_t][a_t] += G\n",
    "            N[s_t][a_t] +=1\n",
    "                \n",
    "            Q[s_t][a_t] = returns[s_t][a_t] /N[s_t][a_t] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "                 \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[s_t][a] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[s_t][a] = (epsilon / len(ALL_POSSIBLE_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2000, 2010, 2020, 2025, 2030, 2035, 2040, 2045, 2050])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=7\n",
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs = step(action , list(current_state),i)\n",
    "    reward = rewardfun(action,current_state,i)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
